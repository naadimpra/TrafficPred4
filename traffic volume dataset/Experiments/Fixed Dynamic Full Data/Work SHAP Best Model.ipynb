{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- weather_description_light rain and snow\n- weather_description_proximity thunderstorm with drizzle\n- weather_description_shower drizzle\n- weather_description_shower snow\n- weather_description_sleet\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 104\u001B[0m\n\u001B[0;32m     98\u001B[0m df \u001B[38;5;241m=\u001B[39m df\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m    100\u001B[0m \u001B[38;5;66;03m# Check categories in encoder\u001B[39;00m\n\u001B[0;32m    101\u001B[0m \u001B[38;5;66;03m# print(scaler.get_feature_names_out())\u001B[39;00m\n\u001B[0;32m    102\u001B[0m \n\u001B[0;32m    103\u001B[0m \u001B[38;5;66;03m# Scale the numerical features\u001B[39;00m\n\u001B[1;32m--> 104\u001B[0m df_scaled \u001B[38;5;241m=\u001B[39m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Use the previously loaded scaler\u001B[39;00m\n\u001B[0;32m    106\u001B[0m \u001B[38;5;66;03m# Convert scaled data back to DataFrame\u001B[39;00m\n\u001B[0;32m    107\u001B[0m df_scaled \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(df_scaled, columns\u001B[38;5;241m=\u001B[39m[col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;28;01mif\u001B[39;00m col \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    163\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1006\u001B[0m, in \u001B[0;36mStandardScaler.transform\u001B[1;34m(self, X, copy)\u001B[0m\n\u001B[0;32m   1003\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m   1005\u001B[0m copy \u001B[38;5;241m=\u001B[39m copy \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy\n\u001B[1;32m-> 1006\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1007\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1008\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1009\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1010\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1011\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1013\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sparse\u001B[38;5;241m.\u001B[39missparse(X):\n\u001B[0;32m   1016\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwith_mean:\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\base.py:580\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_data\u001B[39m(\n\u001B[0;32m    510\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    511\u001B[0m     X\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params,\n\u001B[0;32m    517\u001B[0m ):\n\u001B[0;32m    518\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001B[39;00m\n\u001B[0;32m    519\u001B[0m \n\u001B[0;32m    520\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    578\u001B[0m \u001B[38;5;124;03m        validated.\u001B[39;00m\n\u001B[0;32m    579\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 580\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_feature_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    582\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_tags()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires_y\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    583\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    584\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m estimator \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    585\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires y to be passed, but the target y is None.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    586\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\base.py:507\u001B[0m, in \u001B[0;36mBaseEstimator._check_feature_names\u001B[1;34m(self, X, reset)\u001B[0m\n\u001B[0;32m    502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m missing_names \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m unexpected_names:\n\u001B[0;32m    503\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature names must be in the same order as they were in fit.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    505\u001B[0m     )\n\u001B[1;32m--> 507\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n",
      "\u001B[1;31mValueError\u001B[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- weather_description_light rain and snow\n- weather_description_proximity thunderstorm with drizzle\n- weather_description_shower drizzle\n- weather_description_shower snow\n- weather_description_sleet\n- ...\n"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "df_raw = pd.read_csv('../../Train.csv')\n",
    "encoder = joblib.load('encoder.joblib')\n",
    "# Convert the 'date_time' column to datetime and sort the dataset\n",
    "df_raw['date_time'] = pd.to_datetime(df_raw['date_time'])\n",
    "df_raw.sort_values('date_time', inplace=True)\n",
    "\n",
    "# Extracting non-numeric columns\n",
    "non_numeric_cols = ['is_holiday', 'weather_type', 'weather_description']\n",
    "\n",
    "# Group by 'date_time' and aggregate: mean for numeric columns, mode for non-numeric columns\n",
    "agg_funcs = {col: 'mean' for col in df_raw.columns if col not in non_numeric_cols}\n",
    "agg_funcs.update({col: lambda x: x.mode()[0] if not x.mode().empty else np.nan for col in non_numeric_cols})\n",
    "\n",
    "df_aggregated = df_raw.groupby('date_time').agg(agg_funcs)\n",
    "\n",
    "# Extract unique values for categorical columns from df_raw\n",
    "unique_values = {col: df_raw[col].unique() for col in non_numeric_cols}\n",
    "\n",
    "# One-hot encode categorical features using unique values\n",
    "encoded_data = encoder.transform(df_aggregated[non_numeric_cols])\n",
    "\n",
    "# Create a DataFrame with encoded data and columns\n",
    "df_encode = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# Ensure all unique values in df_raw are included in df_encode\n",
    "for col in non_numeric_cols:\n",
    "    for value in unique_values[col]:\n",
    "        column_name = f\"{col}_{value}\"\n",
    "        if column_name not in df_encode.columns:\n",
    "            df_encode[column_name] = 0  # Add missing column with zeros\n",
    "\n",
    "# Reset index of df_encode\n",
    "df_encode.index = df_aggregated.index\n",
    "\n",
    "# Concatenate with df_aggregated\n",
    "df = pd.concat([df_aggregated, df_encode], axis=1)\n",
    "\n",
    "# Add hour from the 'date_time' column\n",
    "df['hour'] = df.index.hour\n",
    "\n",
    "# Feature engineering: create lagged and rolling features\n",
    "target = 'traffic_volume'\n",
    "for i in range(1, 4):\n",
    "    df[f'traffic_volume_lag_{i}'] = df[target].shift(i)\n",
    "df['traffic_volume_rolling_mean'] = df[target].rolling(window=3).mean().shift(1)\n",
    "df['traffic_volume_rolling_std'] = df[target].rolling(window=3).std().shift(1)\n",
    "\n",
    "# Remove rows with NaN values resulting from lagged features\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the dataset into features and the target\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Save 'date_time' for later use\n",
    "date_time = df['date_time']\n",
    "\n",
    "# Drop 'date_time' column before scaling\n",
    "df = df.drop(columns=['date_time'])\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = joblib.load('scaler2.joblib')\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Convert scaled data back to DataFrame and add 'date_time' column back\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=[col for col in df.columns if col != 'date_time'])\n",
    "df_scaled['date_time'] = date_time.values\n",
    "\n",
    "# Split the data into train and test sets\n",
    "total_samples = df_scaled.shape[0]\n",
    "split_index = int(total_samples * 0.9)\n",
    "\n",
    "X_train = df_scaled.iloc[:split_index].drop(columns=['date_time'])\n",
    "y_train = y.iloc[:split_index]\n",
    "X_test = df_scaled.iloc[split_index:].drop(columns=['date_time'])\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "X_train = X_train.drop(target, axis=1)\n",
    "X_test = X_test.drop(target, axis=1)\n",
    "\n",
    "best_model_grid = 'best_xgboost_model_gridsearch.joblib'\n",
    "\n",
    "# Create a SHAP Tree Explainer for the XGBoost model\n",
    "explainer = shap.TreeExplainer(joblib.load(best_model_grid))\n",
    "\n",
    "# Calculate SHAP values - this might take some time for larger datasets\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "# Mean absolute SHAP values\n",
    "shap.summary_plot(shap_values, X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-01T11:17:04.250327400Z",
     "start_time": "2024-01-01T11:16:47.879046500Z"
    }
   },
   "id": "32d9ed00e6ca3915"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Force plot for a single prediction\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X_test.iloc[0, :])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T15:05:16.590248500Z"
    }
   },
   "id": "58c8fc8d4c85e953"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- weather_description_light rain and snow\n- weather_description_proximity thunderstorm with drizzle\n- weather_description_shower drizzle\n- weather_description_shower snow\n- weather_description_sleet\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 89\u001B[0m\n\u001B[0;32m     84\u001B[0m scaler \u001B[38;5;241m=\u001B[39m joblib\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscaler2.joblib\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     85\u001B[0m \u001B[38;5;66;03m# Check categories in encoder\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# print(scaler.get_feature_names_out())\u001B[39;00m\n\u001B[0;32m     87\u001B[0m \n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# Scale the numerical features\u001B[39;00m\n\u001B[1;32m---> 89\u001B[0m df_scaled \u001B[38;5;241m=\u001B[39m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Use the previously loaded scaler\u001B[39;00m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# Convert scaled data back to DataFrame\u001B[39;00m\n\u001B[0;32m     92\u001B[0m df_scaled \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(df_scaled, columns\u001B[38;5;241m=\u001B[39m[col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;28;01mif\u001B[39;00m col \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    163\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1006\u001B[0m, in \u001B[0;36mStandardScaler.transform\u001B[1;34m(self, X, copy)\u001B[0m\n\u001B[0;32m   1003\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m   1005\u001B[0m copy \u001B[38;5;241m=\u001B[39m copy \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy\n\u001B[1;32m-> 1006\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1007\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1008\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1009\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1010\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1011\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1013\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sparse\u001B[38;5;241m.\u001B[39missparse(X):\n\u001B[0;32m   1016\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwith_mean:\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\base.py:580\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_data\u001B[39m(\n\u001B[0;32m    510\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    511\u001B[0m     X\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params,\n\u001B[0;32m    517\u001B[0m ):\n\u001B[0;32m    518\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001B[39;00m\n\u001B[0;32m    519\u001B[0m \n\u001B[0;32m    520\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    578\u001B[0m \u001B[38;5;124;03m        validated.\u001B[39;00m\n\u001B[0;32m    579\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 580\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_feature_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    582\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_tags()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires_y\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    583\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    584\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m estimator \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    585\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires y to be passed, but the target y is None.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    586\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\base.py:507\u001B[0m, in \u001B[0;36mBaseEstimator._check_feature_names\u001B[1;34m(self, X, reset)\u001B[0m\n\u001B[0;32m    502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m missing_names \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m unexpected_names:\n\u001B[0;32m    503\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature names must be in the same order as they were in fit.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    505\u001B[0m     )\n\u001B[1;32m--> 507\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n",
      "\u001B[1;31mValueError\u001B[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- weather_description_light rain and snow\n- weather_description_proximity thunderstorm with drizzle\n- weather_description_shower drizzle\n- weather_description_shower snow\n- weather_description_sleet\n- ...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the encoder, scaler, and model\n",
    "encoder = joblib.load('encoder.joblib')\n",
    "scaler = joblib.load('scaler2.joblib')\n",
    "model = joblib.load('best_xgboost_model_gridsearch.joblib')\n",
    "\n",
    "# Load the dataset\n",
    "df_raw = pd.read_csv('../../Train.csv')\n",
    "\n",
    "# Convert the 'date_time' column to datetime and sort the dataset\n",
    "df_raw['date_time'] = pd.to_datetime(df_raw['date_time'])\n",
    "df_raw.sort_values('date_time', inplace=True)\n",
    "\n",
    "# Extracting non-numeric columns\n",
    "non_numeric_cols = ['is_holiday', 'weather_type', 'weather_description']\n",
    "\n",
    "# for col in non_numeric_cols:\n",
    "#     print(f\"Unique values for {col} in df_raw: {df_raw[col].unique()}\")\n",
    "\n",
    "\n",
    "# Group by 'date_time' and aggregate: mean for numeric columns, mode for non-numeric columns\n",
    "agg_funcs = {col: 'mean' for col in df_raw.columns if col not in non_numeric_cols}\n",
    "agg_funcs.update({col: lambda x: x.mode()[0] if not x.mode().empty else np.nan for col in non_numeric_cols})\n",
    "\n",
    "df_aggregated = df_raw.groupby('date_time').agg(agg_funcs)\n",
    "\n",
    "\n",
    "# Extract unique values for categorical columns from df_raw\n",
    "unique_values = {col: df_raw[col].unique() for col in non_numeric_cols}\n",
    "\n",
    "# One-hot encode categorical features using unique values\n",
    "encoded_data = encoder.transform(df_aggregated[non_numeric_cols])\n",
    "\n",
    "# Create a DataFrame with encoded data and columns\n",
    "df_encode = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# Ensure all unique values in df_raw are included in df_encode\n",
    "for col in non_numeric_cols:\n",
    "    for value in unique_values[col]:\n",
    "        column_name = f\"{col}_{value}\"\n",
    "        if column_name not in df_encode.columns:\n",
    "            df_encode[column_name] = 0  # Add missing column with zeros\n",
    "\n",
    "# Reset index of df_encode\n",
    "df_encode.index = df_aggregated.index\n",
    "\n",
    "# Concatenate with df_aggregated\n",
    "df = pd.concat([df_aggregated, df_encode], axis=1)\n",
    "\n",
    "# Add hour from the 'date_time' column\n",
    "df['hour'] = df['date_time'].dt.hour\n",
    "df = df.drop(columns=non_numeric_cols)\n",
    "\n",
    "# Feature engineering: create lagged and rolling features\n",
    "target = 'traffic_volume'\n",
    "for i in range(1, 4):\n",
    "    df[f'traffic_volume_lag_{i}'] = df[target].shift(i)\n",
    "df['traffic_volume_rolling_mean'] = df[target].rolling(window=3).mean().shift(1)\n",
    "df['traffic_volume_rolling_std'] = df[target].rolling(window=3).std().shift(1)\n",
    "\n",
    "# Remove rows with NaN values resulting from lagged features\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the dataset into features and the target\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Save 'date_time' for later use\n",
    "date_time = df['date_time']\n",
    "\n",
    "# Drop 'date_time' column before scaling\n",
    "df = df.drop(columns=['date_time'])\n",
    "\n",
    "scaler = joblib.load('scaler2.joblib')\n",
    "# Check categories in encoder\n",
    "# print(scaler.get_feature_names_out())\n",
    "\n",
    "# Scale the numerical features\n",
    "df_scaled = scaler.transform(df)  # Use the previously loaded scaler\n",
    "\n",
    "# Convert scaled data back to DataFrame\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=[col for col in df.columns if col != 'date_time'])\n",
    "df_scaled['date_time'] = date_time.values\n",
    "\n",
    "X = df_scaled.drop(columns=['date_time'])\n",
    "\n",
    "X = X.drop(target, axis=1)\n",
    "best_model_grid = 'best_xgboost_model_gridsearch.joblib'\n",
    "\n",
    "# Create a SHAP Tree Explainer for the XGBoost model\n",
    "explainer = shap.TreeExplainer(joblib.load(best_model_grid))\n",
    "\n",
    "# Calculate SHAP values - this might take some time for larger datasets\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# Force plot for a single prediction\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X.iloc[0, :])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T05:52:36.740803300Z",
     "start_time": "2024-01-02T05:52:15.131526700Z"
    }
   },
   "id": "813e464c77e3e0c8"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nadim Pramono\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "encoder = joblib.load('encoderbaru.joblib')\n",
    "scaler = joblib.load('scalerbaru.joblib')\n",
    "model = joblib.load('best_xgboost_model.joblib')\n",
    "\n",
    "# Define custom scorer for MAPE\n",
    "def mape_scorer(y_true, y_pred):\n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    non_zero_mask = y_true != 0\n",
    "    return np.mean(np.abs((y_true[non_zero_mask] - y_pred[non_zero_mask]) / y_true[non_zero_mask])) * 100\n",
    "\n",
    "\n",
    "# Make scorers from custom scoring functions\n",
    "mape = make_scorer(mape_scorer, greater_is_better=False)\n",
    "# Load the dataset\n",
    "df_raw = pd.read_csv('../../Train.csv')\n",
    "\n",
    "# Convert the 'date_time' column to datetime and sort the dataset\n",
    "df_raw['date_time'] = pd.to_datetime(df_raw['date_time'])\n",
    "df_raw.sort_values('date_time', inplace=True)\n",
    "\n",
    "# Extracting non-numeric columns\n",
    "non_numeric_cols = ['is_holiday', 'weather_type', 'weather_description']\n",
    "\n",
    "# Group by 'date_time' and aggregate: mean for numeric columns, mode for non-numeric columns\n",
    "agg_funcs = {col: 'mean' for col in df_raw.columns if col not in non_numeric_cols}\n",
    "agg_funcs.update({col: lambda x: x.mode()[0] if not x.mode().empty else np.nan for col in non_numeric_cols})\n",
    "\n",
    "df_aggregated = df_raw.groupby('date_time').agg(agg_funcs)\n",
    "\n",
    "# One-hot encode categorical features\n",
    "encoded_data = encoder.fit_transform(df_aggregated[non_numeric_cols])\n",
    "\n",
    "df_encode = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# Reset index of df_encode\n",
    "df_encode.index = df_aggregated.index\n",
    "\n",
    "# Concatenate with df_aggregated\n",
    "df = pd.concat([df_aggregated, df_encode], axis=1)\n",
    "\n",
    "# Add hour from the 'date_time' column\n",
    "df['hour'] = df['date_time'].dt.hour\n",
    "df = df.drop(columns=non_numeric_cols)\n",
    "# Feature engineering: create lagged and rolling features\n",
    "target = 'traffic_volume'\n",
    "for i in range(1, 4):\n",
    "    df[f'traffic_volume_lag_{i}'] = df[target].shift(i)\n",
    "df['traffic_volume_rolling_mean'] = df[target].rolling(window=3).mean().shift(1)\n",
    "df['traffic_volume_rolling_std'] = df[target].rolling(window=3).std().shift(1)\n",
    "\n",
    "# Remove rows with NaN values resulting from lagged features\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the dataset into features and the target\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Save 'date_time' for later use\n",
    "date_time = df['date_time']\n",
    "\n",
    "# Drop 'date_time' column before scaling\n",
    "df = df.drop(columns=['date_time'])\n",
    "\n",
    "# Scale the numerical features\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Convert scaled data back to DataFrame and add 'date_time' column back\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=[col for col in df.columns if col != 'date_time'])\n",
    "df_scaled['date_time'] = date_time.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T05:55:49.642102800Z",
     "start_time": "2024-01-02T05:55:31.398931900Z"
    }
   },
   "id": "fdaf008ca6aa5a7b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[12:57:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07f6e447eee219473-1\\xgboost\\xgboost-ci-windows\\src\\c_api\\c_api.cc:1240: Saving into deprecated binary model format, please consider using `json` or `ubj`. Model format will default to JSON in XGBoost 2.2 if not specified.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:date_time: datetime64[ns]",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 14\u001B[0m\n\u001B[0;32m     11\u001B[0m explainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mTreeExplainer(model)\n\u001B[0;32m     13\u001B[0m     \u001B[38;5;66;03m# Calculate SHAP values - this might take some time for larger datasets\u001B[39;00m\n\u001B[1;32m---> 14\u001B[0m shap_values \u001B[38;5;241m=\u001B[39m \u001B[43mexplainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshap_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\shap\\explainers\\_tree.py:358\u001B[0m, in \u001B[0;36mTreeExplainer.shap_values\u001B[1;34m(self, X, y, tree_limit, approximate, check_additivity, from_call)\u001B[0m\n\u001B[0;32m    356\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mxgboost\u001B[39;00m\n\u001B[0;32m    357\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(X, xgboost\u001B[38;5;241m.\u001B[39mcore\u001B[38;5;241m.\u001B[39mDMatrix):\n\u001B[1;32m--> 358\u001B[0m     X \u001B[38;5;241m=\u001B[39m \u001B[43mxgboost\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDMatrix\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    359\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tree_limit \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    360\u001B[0m     tree_limit \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\xgboost\\core.py:729\u001B[0m, in \u001B[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    727\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k, arg \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(sig\u001B[38;5;241m.\u001B[39mparameters, args):\n\u001B[0;32m    728\u001B[0m     kwargs[k] \u001B[38;5;241m=\u001B[39m arg\n\u001B[1;32m--> 729\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\xgboost\\core.py:856\u001B[0m, in \u001B[0;36mDMatrix.__init__\u001B[1;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical, data_split_mode)\u001B[0m\n\u001B[0;32m    853\u001B[0m     \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    854\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 856\u001B[0m handle, feature_names, feature_types \u001B[38;5;241m=\u001B[39m \u001B[43mdispatch_data_backend\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    857\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    858\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmissing\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    859\u001B[0m \u001B[43m    \u001B[49m\u001B[43mthreads\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnthread\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    860\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_names\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    861\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeature_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeature_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    862\u001B[0m \u001B[43m    \u001B[49m\u001B[43menable_categorical\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable_categorical\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    863\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_split_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_split_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    864\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    865\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m handle \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    866\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mhandle \u001B[38;5;241m=\u001B[39m handle\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\xgboost\\data.py:1089\u001B[0m, in \u001B[0;36mdispatch_data_backend\u001B[1;34m(data, missing, threads, feature_names, feature_types, enable_categorical, data_split_mode)\u001B[0m\n\u001B[0;32m   1087\u001B[0m     data \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(data)\n\u001B[0;32m   1088\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_pandas_df(data):\n\u001B[1;32m-> 1089\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_from_pandas_df\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1090\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menable_categorical\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmissing\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mthreads\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_types\u001B[49m\n\u001B[0;32m   1091\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1092\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_cudf_df(data) \u001B[38;5;129;01mor\u001B[39;00m _is_cudf_ser(data):\n\u001B[0;32m   1093\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _from_cudf_df(\n\u001B[0;32m   1094\u001B[0m         data, missing, threads, feature_names, feature_types, enable_categorical\n\u001B[0;32m   1095\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\xgboost\\data.py:522\u001B[0m, in \u001B[0;36m_from_pandas_df\u001B[1;34m(data, enable_categorical, missing, nthread, feature_names, feature_types)\u001B[0m\n\u001B[0;32m    514\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_from_pandas_df\u001B[39m(\n\u001B[0;32m    515\u001B[0m     data: DataFrame,\n\u001B[0;32m    516\u001B[0m     enable_categorical: \u001B[38;5;28mbool\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    520\u001B[0m     feature_types: Optional[FeatureTypes],\n\u001B[0;32m    521\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DispatchedDataBackendReturnType:\n\u001B[1;32m--> 522\u001B[0m     data, feature_names, feature_types \u001B[38;5;241m=\u001B[39m \u001B[43m_transform_pandas_df\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43menable_categorical\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_names\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfeature_types\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    525\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _from_numpy_array(data, missing, nthread, feature_names, feature_types)\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\xgboost\\data.py:490\u001B[0m, in \u001B[0;36m_transform_pandas_df\u001B[1;34m(data, enable_categorical, feature_names, feature_types, meta, meta_type)\u001B[0m\n\u001B[0;32m    483\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m dtype \u001B[38;5;129;01min\u001B[39;00m data\u001B[38;5;241m.\u001B[39mdtypes:\n\u001B[0;32m    484\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\n\u001B[0;32m    485\u001B[0m         (dtype\u001B[38;5;241m.\u001B[39mname \u001B[38;5;129;01min\u001B[39;00m _pandas_dtype_mapper)\n\u001B[0;32m    486\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m is_pd_sparse_dtype(dtype)\n\u001B[0;32m    487\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m (is_pd_cat_dtype(dtype) \u001B[38;5;129;01mand\u001B[39;00m enable_categorical)\n\u001B[0;32m    488\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m is_pa_ext_dtype(dtype)\n\u001B[0;32m    489\u001B[0m     ):\n\u001B[1;32m--> 490\u001B[0m         \u001B[43m_invalid_dataframe_dtype\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    491\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_pa_ext_dtype(dtype):\n\u001B[0;32m    492\u001B[0m         pyarrow_extension \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\xgboost\\data.py:308\u001B[0m, in \u001B[0;36m_invalid_dataframe_dtype\u001B[1;34m(data)\u001B[0m\n\u001B[0;32m    306\u001B[0m type_err \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrame.dtypes for data must be int, float, bool or category.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    307\u001B[0m msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\"\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtype_err\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m_ENABLE_CAT_ERR\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;132;01m{\u001B[39;00merr\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\"\"\u001B[39m\n\u001B[1;32m--> 308\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg)\n",
      "\u001B[1;31mValueError\u001B[0m: DataFrame.dtypes for data must be int, float, bool or category. When categorical type is supplied, The experimental DMatrix parameter`enable_categorical` must be set to `True`.  Invalid columns:date_time: datetime64[ns]"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "# Create a SHAP Tree Explainer for the XGBoost model\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "    # Calculate SHAP values - this might take some time for larger datasets\n",
    "shap_values = explainer.shap_values(X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-02T05:57:31.536749400Z",
     "start_time": "2024-01-02T05:57:26.665335400Z"
    }
   },
   "id": "e696e3fd0e380957"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "462d747b3d8412fa"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
