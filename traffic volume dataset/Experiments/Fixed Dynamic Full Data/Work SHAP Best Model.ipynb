{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'None'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_14248\\2540925226.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[1;32m---> 74\u001B[1;33m \u001B[1;32mimport\u001B[0m \u001B[0mshap\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     75\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     76\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     77\u001B[0m \u001B[1;32mimport\u001B[0m \u001B[0mxgboost\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mxgb\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\utils\\_set_output.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    155\u001B[0m     \u001B[1;33m@\u001B[0m\u001B[0mwraps\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 157\u001B[1;33m         \u001B[0mdata_to_wrap\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mf\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    158\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0misinstance\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_to_wrap\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    159\u001B[0m             \u001B[1;31m# only wrap the first output for cross decomposition\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    160\u001B[0m             return_tuple = (\n",
      "\u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\base.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    912\u001B[0m         \u001B[1;31m# non-optimized default implementation; override when a better\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    913\u001B[0m         \u001B[1;31m# method is possible for a given clustering algorithm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    914\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0my\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    915\u001B[0m             \u001B[1;31m# fit method of arity 1 (unsupervised transformation)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 916\u001B[1;33m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    917\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    918\u001B[0m             \u001B[1;31m# fit method of arity 2 (supervised transformation)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    919\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    835\u001B[0m             \u001B[0mFitted\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    836\u001B[0m         \"\"\"\n\u001B[0;32m    837\u001B[0m         \u001B[1;31m# Reset internal state before fitting\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    838\u001B[0m         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_reset\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 839\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpartial_fit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0msample_weight\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\base.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1148\u001B[0m                 skip_parameter_validation=(\n\u001B[0;32m   1149\u001B[0m                     \u001B[0mprefer_skip_nested_validation\u001B[0m \u001B[1;32mor\u001B[0m \u001B[0mglobal_skip_validation\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1150\u001B[0m                 )\n\u001B[0;32m   1151\u001B[0m             ):\n\u001B[1;32m-> 1152\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfit_method\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mestimator\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\preprocessing\\_data.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[0;32m    871\u001B[0m         \u001B[0mself\u001B[0m \u001B[1;33m:\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    872\u001B[0m             \u001B[0mFitted\u001B[0m \u001B[0mscaler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    873\u001B[0m         \"\"\"\n\u001B[0;32m    874\u001B[0m         \u001B[0mfirst_call\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"n_samples_seen_\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 875\u001B[1;33m         X = self._validate_data(\n\u001B[0m\u001B[0;32m    876\u001B[0m             \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    877\u001B[0m             \u001B[0maccept_sparse\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"csr\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m\"csc\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    878\u001B[0m             \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mFLOAT_DTYPES\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\base.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    601\u001B[0m                 \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    602\u001B[0m             \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    603\u001B[0m                 \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0my\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    604\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mno_val_X\u001B[0m \u001B[1;32mand\u001B[0m \u001B[0mno_val_y\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 605\u001B[1;33m             \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mcheck_array\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mX\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minput_name\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;34m\"X\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    606\u001B[0m         \u001B[1;32melif\u001B[0m \u001B[0mno_val_X\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mno_val_y\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    607\u001B[0m             \u001B[0mout\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_check_y\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0my\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mcheck_params\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    608\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\utils\\validation.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    913\u001B[0m                     \u001B[0marray\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mxp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcopy\u001B[0m\u001B[1;33m=\u001B[0m\u001B[1;32mFalse\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    914\u001B[0m                 \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    915\u001B[0m                     \u001B[0marray\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_asarray_with_order\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0morder\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0morder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mxp\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mxp\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    916\u001B[0m             \u001B[1;32mexcept\u001B[0m \u001B[0mComplexWarning\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mcomplex_warning\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 917\u001B[1;33m                 raise ValueError(\n\u001B[0m\u001B[0;32m    918\u001B[0m                     \u001B[1;34m\"Complex data not supported\\n{}\\n\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    919\u001B[0m                 ) from complex_warning\n\u001B[0;32m    920\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\utils\\_array_api.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(array, dtype, order, copy, xp)\u001B[0m\n\u001B[0;32m    376\u001B[0m         \u001B[1;31m# Use NumPy API to support order\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    377\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0mcopy\u001B[0m \u001B[1;32mis\u001B[0m \u001B[1;32mTrue\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    378\u001B[0m             \u001B[0marray\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0morder\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0morder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    379\u001B[0m         \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 380\u001B[1;33m             \u001B[0marray\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mnumpy\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0marray\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0morder\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0morder\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    381\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    382\u001B[0m         \u001B[1;31m# At this point array is a NumPy ndarray. We convert it to an array\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    383\u001B[0m         \u001B[1;31m# container that is consistent with the input's namespace.\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, dtype)\u001B[0m\n\u001B[0;32m   2069\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__array__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mnpt\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mDTypeLike\u001B[0m \u001B[1;33m|\u001B[0m \u001B[1;32mNone\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mNone\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;33m->\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mndarray\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 2070\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mnp\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0masarray\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_values\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: 'None'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "\n",
    "# Load the dataset\n",
    "df_raw = pd.read_csv('../../Train.csv')\n",
    "encoder = joblib.load('encoder.joblib')\n",
    "# Convert the 'date_time' column to datetime and sort the dataset\n",
    "df_raw['date_time'] = pd.to_datetime(df_raw['date_time'])\n",
    "df_raw.sort_values('date_time', inplace=True)\n",
    "\n",
    "# Extracting non-numeric columns\n",
    "non_numeric_cols = ['is_holiday', 'weather_type', 'weather_description']\n",
    "\n",
    "# Group by 'date_time' and aggregate: mean for numeric columns, mode for non-numeric columns\n",
    "agg_funcs = {col: 'mean' for col in df_raw.columns if col not in non_numeric_cols}\n",
    "agg_funcs.update({col: lambda x: x.mode()[0] if not x.mode().empty else np.nan for col in non_numeric_cols})\n",
    "\n",
    "df_aggregated = df_raw.groupby('date_time').agg(agg_funcs)\n",
    "\n",
    "# Extract unique values for categorical columns from df_raw\n",
    "unique_values = {col: df_raw[col].unique() for col in non_numeric_cols}\n",
    "\n",
    "# One-hot encode categorical features using unique values\n",
    "encoded_data = encoder.transform(df_aggregated[non_numeric_cols])\n",
    "\n",
    "# Create a DataFrame with encoded data and columns\n",
    "df_encode = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# Ensure all unique values in df_raw are included in df_encode\n",
    "for col in non_numeric_cols:\n",
    "    for value in unique_values[col]:\n",
    "        column_name = f\"{col}_{value}\"\n",
    "        if column_name not in df_encode.columns:\n",
    "            df_encode[column_name] = 0  # Add missing column with zeros\n",
    "\n",
    "# Reset index of df_encode\n",
    "df_encode.index = df_aggregated.index\n",
    "\n",
    "# Concatenate with df_aggregated\n",
    "df = pd.concat([df_aggregated, df_encode], axis=1)\n",
    "\n",
    "# Add hour from the 'date_time' column\n",
    "df['hour'] = df.index.hour\n",
    "\n",
    "# Feature engineering: create lagged and rolling features\n",
    "target = 'traffic_volume'\n",
    "for i in range(1, 4):\n",
    "    df[f'traffic_volume_lag_{i}'] = df[target].shift(i)\n",
    "df['traffic_volume_rolling_mean'] = df[target].rolling(window=3).mean().shift(1)\n",
    "df['traffic_volume_rolling_std'] = df[target].rolling(window=3).std().shift(1)\n",
    "\n",
    "# Remove rows with NaN values resulting from lagged features\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the dataset into features and the target\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Save 'date_time' for later use\n",
    "date_time = df['date_time']\n",
    "\n",
    "# Drop 'date_time' column before scaling\n",
    "df = df.drop(columns=['date_time'])\n",
    "\n",
    "# Scale the numerical features\n",
    "scaler = joblib.load('scaler2.joblib')\n",
    "df_scaled = scaler.fit_transform(df)\n",
    "\n",
    "# Convert scaled data back to DataFrame and add 'date_time' column back\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=[col for col in df.columns if col != 'date_time'])\n",
    "df_scaled['date_time'] = date_time.values\n",
    "\n",
    "# Split the data into train and test sets\n",
    "total_samples = df_scaled.shape[0]\n",
    "split_index = int(total_samples * 0.9)\n",
    "\n",
    "X_train = df_scaled.iloc[:split_index].drop(columns=['date_time'])\n",
    "y_train = y.iloc[:split_index]\n",
    "X_test = df_scaled.iloc[split_index:].drop(columns=['date_time'])\n",
    "y_test = y.iloc[split_index:]\n",
    "\n",
    "X_train = X_train.drop(target, axis=1)\n",
    "X_test = X_test.drop(target, axis=1)\n",
    "\n",
    "best_model_grid = 'best_xgboost_model_gridsearch.joblib'\n",
    "\n",
    "# Create a SHAP Tree Explainer for the XGBoost model\n",
    "explainer = shap.TreeExplainer(joblib.load(best_model_grid))\n",
    "\n",
    "# Calculate SHAP values - this might take some time for larger datasets\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "# Mean absolute SHAP values\n",
    "shap.summary_plot(shap_values, X_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T15:17:33.696343Z",
     "start_time": "2023-12-26T15:17:15.042169200Z"
    }
   },
   "id": "32d9ed00e6ca3915"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Force plot for a single prediction\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X_test.iloc[0, :])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T15:05:16.590248500Z"
    }
   },
   "id": "58c8fc8d4c85e953"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- weather_description_light rain and snow\n- weather_description_proximity thunderstorm with drizzle\n- weather_description_shower drizzle\n- weather_description_shower snow\n- weather_description_sleet\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 89\u001B[0m\n\u001B[0;32m     84\u001B[0m scaler \u001B[38;5;241m=\u001B[39m joblib\u001B[38;5;241m.\u001B[39mload(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscaler2.joblib\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     85\u001B[0m \u001B[38;5;66;03m# Check categories in encoder\u001B[39;00m\n\u001B[0;32m     86\u001B[0m \u001B[38;5;66;03m# print(scaler.get_feature_names_out())\u001B[39;00m\n\u001B[0;32m     87\u001B[0m \n\u001B[0;32m     88\u001B[0m \u001B[38;5;66;03m# Scale the numerical features\u001B[39;00m\n\u001B[1;32m---> 89\u001B[0m df_scaled \u001B[38;5;241m=\u001B[39m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Use the previously loaded scaler\u001B[39;00m\n\u001B[0;32m     91\u001B[0m \u001B[38;5;66;03m# Convert scaled data back to DataFrame\u001B[39;00m\n\u001B[0;32m     92\u001B[0m df_scaled \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(df_scaled, columns\u001B[38;5;241m=\u001B[39m[col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;28;01mif\u001B[39;00m col \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    163\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1006\u001B[0m, in \u001B[0;36mStandardScaler.transform\u001B[1;34m(self, X, copy)\u001B[0m\n\u001B[0;32m   1003\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m   1005\u001B[0m copy \u001B[38;5;241m=\u001B[39m copy \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy\n\u001B[1;32m-> 1006\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1007\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1008\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1009\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1010\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1011\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1013\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sparse\u001B[38;5;241m.\u001B[39missparse(X):\n\u001B[0;32m   1016\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwith_mean:\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\base.py:580\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_data\u001B[39m(\n\u001B[0;32m    510\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    511\u001B[0m     X\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params,\n\u001B[0;32m    517\u001B[0m ):\n\u001B[0;32m    518\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001B[39;00m\n\u001B[0;32m    519\u001B[0m \n\u001B[0;32m    520\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    578\u001B[0m \u001B[38;5;124;03m        validated.\u001B[39;00m\n\u001B[0;32m    579\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 580\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_feature_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    582\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_tags()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires_y\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    583\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    584\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m estimator \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    585\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires y to be passed, but the target y is None.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    586\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\base.py:507\u001B[0m, in \u001B[0;36mBaseEstimator._check_feature_names\u001B[1;34m(self, X, reset)\u001B[0m\n\u001B[0;32m    502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m missing_names \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m unexpected_names:\n\u001B[0;32m    503\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature names must be in the same order as they were in fit.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    505\u001B[0m     )\n\u001B[1;32m--> 507\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n",
      "\u001B[1;31mValueError\u001B[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- weather_description_light rain and snow\n- weather_description_proximity thunderstorm with drizzle\n- weather_description_shower drizzle\n- weather_description_shower snow\n- weather_description_sleet\n- ...\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the encoder, scaler, and model\n",
    "encoder = joblib.load('encoder.joblib')\n",
    "scaler = joblib.load('scaler2.joblib')\n",
    "model = joblib.load('best_xgboost_model_gridsearch.joblib')\n",
    "\n",
    "# Load the dataset\n",
    "df_raw = pd.read_csv('../../Train.csv')\n",
    "\n",
    "# Convert the 'date_time' column to datetime and sort the dataset\n",
    "df_raw['date_time'] = pd.to_datetime(df_raw['date_time'])\n",
    "df_raw.sort_values('date_time', inplace=True)\n",
    "\n",
    "# Extracting non-numeric columns\n",
    "non_numeric_cols = ['is_holiday', 'weather_type', 'weather_description']\n",
    "\n",
    "# for col in non_numeric_cols:\n",
    "#     print(f\"Unique values for {col} in df_raw: {df_raw[col].unique()}\")\n",
    "\n",
    "\n",
    "# Group by 'date_time' and aggregate: mean for numeric columns, mode for non-numeric columns\n",
    "agg_funcs = {col: 'mean' for col in df_raw.columns if col not in non_numeric_cols}\n",
    "agg_funcs.update({col: lambda x: x.mode()[0] if not x.mode().empty else np.nan for col in non_numeric_cols})\n",
    "\n",
    "df_aggregated = df_raw.groupby('date_time').agg(agg_funcs)\n",
    "\n",
    "\n",
    "# Extract unique values for categorical columns from df_raw\n",
    "unique_values = {col: df_raw[col].unique() for col in non_numeric_cols}\n",
    "\n",
    "# One-hot encode categorical features using unique values\n",
    "encoded_data = encoder.transform(df_aggregated[non_numeric_cols])\n",
    "\n",
    "# Create a DataFrame with encoded data and columns\n",
    "df_encode = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out())\n",
    "\n",
    "# Ensure all unique values in df_raw are included in df_encode\n",
    "for col in non_numeric_cols:\n",
    "    for value in unique_values[col]:\n",
    "        column_name = f\"{col}_{value}\"\n",
    "        if column_name not in df_encode.columns:\n",
    "            df_encode[column_name] = 0  # Add missing column with zeros\n",
    "\n",
    "# Reset index of df_encode\n",
    "df_encode.index = df_aggregated.index\n",
    "\n",
    "# Concatenate with df_aggregated\n",
    "df = pd.concat([df_aggregated, df_encode], axis=1)\n",
    "\n",
    "# Add hour from the 'date_time' column\n",
    "df['hour'] = df['date_time'].dt.hour\n",
    "df = df.drop(columns=non_numeric_cols)\n",
    "\n",
    "# Feature engineering: create lagged and rolling features\n",
    "target = 'traffic_volume'\n",
    "for i in range(1, 4):\n",
    "    df[f'traffic_volume_lag_{i}'] = df[target].shift(i)\n",
    "df['traffic_volume_rolling_mean'] = df[target].rolling(window=3).mean().shift(1)\n",
    "df['traffic_volume_rolling_std'] = df[target].rolling(window=3).std().shift(1)\n",
    "\n",
    "# Remove rows with NaN values resulting from lagged features\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the dataset into features and the target\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Save 'date_time' for later use\n",
    "date_time = df['date_time']\n",
    "\n",
    "# Drop 'date_time' column before scaling\n",
    "df = df.drop(columns=['date_time'])\n",
    "\n",
    "scaler = joblib.load('scaler2.joblib')\n",
    "# Check categories in encoder\n",
    "# print(scaler.get_feature_names_out())\n",
    "\n",
    "# Scale the numerical features\n",
    "df_scaled = scaler.transform(df)  # Use the previously loaded scaler\n",
    "\n",
    "# Convert scaled data back to DataFrame\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=[col for col in df.columns if col != 'date_time'])\n",
    "df_scaled['date_time'] = date_time.values\n",
    "\n",
    "X = df_scaled.drop(columns=['date_time'])\n",
    "\n",
    "X = X.drop(target, axis=1)\n",
    "best_model_grid = 'best_xgboost_model_gridsearch.joblib'\n",
    "\n",
    "# Create a SHAP Tree Explainer for the XGBoost model\n",
    "explainer = shap.TreeExplainer(joblib.load(best_model_grid))\n",
    "\n",
    "# Calculate SHAP values - this might take some time for larger datasets\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# Force plot for a single prediction\n",
    "shap.force_plot(explainer.expected_value, shap_values[0, :], X.iloc[0, :])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T15:19:21.448461Z",
     "start_time": "2023-12-26T15:19:05.516803600Z"
    }
   },
   "id": "813e464c77e3e0c8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fdaf008ca6aa5a7b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
