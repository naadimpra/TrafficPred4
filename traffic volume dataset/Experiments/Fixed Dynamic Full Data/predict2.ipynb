{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load the encoder, scaler, and model\n",
    "encoder = joblib.load('encoder.joblib')\n",
    "scaler = joblib.load('scaler.joblib')\n",
    "model = joblib.load('best_xgboost_model_gridsearch.joblib')\n",
    "\n",
    "# Load the dataset\n",
    "df_raw = pd.read_csv('../../Train.csv')\n",
    "df_raw = df_raw.tail(10)\n",
    "# Convert the 'date_time' column to datetime and sort the dataset\n",
    "df_raw['date_time'] = pd.to_datetime(df_raw['date_time'])\n",
    "df_raw.sort_values('date_time', inplace=True)\n",
    "\n",
    "# Extracting non-numeric columns\n",
    "non_numeric_cols = ['is_holiday', 'weather_type', 'weather_description']\n",
    "\n",
    "# Group by 'date_time' and aggregate: mean for numeric columns, mode for non-numeric columns\n",
    "agg_funcs = {col: 'mean' for col in df_raw.columns if col not in non_numeric_cols}\n",
    "agg_funcs.update({col: lambda x: x.mode()[0] if not x.mode().empty else np.nan for col in non_numeric_cols})\n",
    "\n",
    "df_aggregated = df_raw.groupby('date_time').agg(agg_funcs)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:56.474338600Z",
     "start_time": "2023-12-26T09:54:56.284140400Z"
    }
   },
   "id": "b2e950341af9dd13"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                              date_time  air_pollution_index  humidity  \\\ndate_time                                                                \n2017-05-17 20:00:00 2017-05-17 20:00:00           173.666667      86.0   \n2017-05-17 21:00:00 2017-05-17 21:00:00           122.666667      85.0   \n2017-05-17 22:00:00 2017-05-17 22:00:00           109.500000      70.0   \n2017-05-17 23:00:00 2017-05-17 23:00:00           184.500000      64.5   \n\n                     wind_speed  wind_direction  visibility_in_miles  \\\ndate_time                                                              \n2017-05-17 20:00:00         1.0           326.0             4.000000   \n2017-05-17 21:00:00         1.0           328.0             6.666667   \n2017-05-17 22:00:00         1.0            24.0             1.500000   \n2017-05-17 23:00:00         1.0            34.5             7.000000   \n\n                     dew_point  temperature  rain_p_h  snow_p_h  clouds_all  \\\ndate_time                                                                     \n2017-05-17 20:00:00   4.000000       288.89       0.0       0.0        90.0   \n2017-05-17 21:00:00   6.666667       287.88       0.0       0.0        90.0   \n2017-05-17 22:00:00   1.500000       286.95       0.0       0.0        90.0   \n2017-05-17 23:00:00   7.000000       285.75       0.0       0.0        90.0   \n\n                     traffic_volume is_holiday weather_type  \\\ndate_time                                                     \n2017-05-17 20:00:00          2733.0       None         Mist   \n2017-05-17 21:00:00          2348.0       None         Mist   \n2017-05-17 22:00:00          2194.0       None         Mist   \n2017-05-17 23:00:00          1328.0       None         Mist   \n\n                      weather_description  \ndate_time                                  \n2017-05-17 20:00:00  heavy intensity rain  \n2017-05-17 21:00:00            light rain  \n2017-05-17 22:00:00  heavy intensity rain  \n2017-05-17 23:00:00  heavy intensity rain  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date_time</th>\n      <th>air_pollution_index</th>\n      <th>humidity</th>\n      <th>wind_speed</th>\n      <th>wind_direction</th>\n      <th>visibility_in_miles</th>\n      <th>dew_point</th>\n      <th>temperature</th>\n      <th>rain_p_h</th>\n      <th>snow_p_h</th>\n      <th>clouds_all</th>\n      <th>traffic_volume</th>\n      <th>is_holiday</th>\n      <th>weather_type</th>\n      <th>weather_description</th>\n    </tr>\n    <tr>\n      <th>date_time</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2017-05-17 20:00:00</th>\n      <td>2017-05-17 20:00:00</td>\n      <td>173.666667</td>\n      <td>86.0</td>\n      <td>1.0</td>\n      <td>326.0</td>\n      <td>4.000000</td>\n      <td>4.000000</td>\n      <td>288.89</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>90.0</td>\n      <td>2733.0</td>\n      <td>None</td>\n      <td>Mist</td>\n      <td>heavy intensity rain</td>\n    </tr>\n    <tr>\n      <th>2017-05-17 21:00:00</th>\n      <td>2017-05-17 21:00:00</td>\n      <td>122.666667</td>\n      <td>85.0</td>\n      <td>1.0</td>\n      <td>328.0</td>\n      <td>6.666667</td>\n      <td>6.666667</td>\n      <td>287.88</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>90.0</td>\n      <td>2348.0</td>\n      <td>None</td>\n      <td>Mist</td>\n      <td>light rain</td>\n    </tr>\n    <tr>\n      <th>2017-05-17 22:00:00</th>\n      <td>2017-05-17 22:00:00</td>\n      <td>109.500000</td>\n      <td>70.0</td>\n      <td>1.0</td>\n      <td>24.0</td>\n      <td>1.500000</td>\n      <td>1.500000</td>\n      <td>286.95</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>90.0</td>\n      <td>2194.0</td>\n      <td>None</td>\n      <td>Mist</td>\n      <td>heavy intensity rain</td>\n    </tr>\n    <tr>\n      <th>2017-05-17 23:00:00</th>\n      <td>2017-05-17 23:00:00</td>\n      <td>184.500000</td>\n      <td>64.5</td>\n      <td>1.0</td>\n      <td>34.5</td>\n      <td>7.000000</td>\n      <td>7.000000</td>\n      <td>285.75</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>90.0</td>\n      <td>1328.0</td>\n      <td>None</td>\n      <td>Mist</td>\n      <td>heavy intensity rain</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_aggregated"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:56.480331400Z",
     "start_time": "2023-12-26T09:54:56.385510800Z"
    }
   },
   "id": "447c70e9d31080b0"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Nadim Pramono\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode categorical features\n",
    "encoded_data = encoder.fit_transform(df_aggregated[non_numeric_cols])\n",
    "\n",
    "df_encode = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:56.483454100Z",
     "start_time": "2023-12-26T09:54:56.415125200Z"
    }
   },
   "id": "59778eaacd28df92"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Reset index of df_encode\n",
    "df_encode.index = df_aggregated.index"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:56.483454100Z",
     "start_time": "2023-12-26T09:54:56.431233600Z"
    }
   },
   "id": "7bcb96d4cda7239"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "Index(['is_holiday_None', 'weather_type_Mist',\n       'weather_description_heavy intensity rain',\n       'weather_description_light rain'],\n      dtype='object')"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_encode.columns"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:56.485003100Z",
     "start_time": "2023-12-26T09:54:56.448164500Z"
    }
   },
   "id": "9f93c890f4cdd9bd"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Concatenate with df_aggregated\n",
    "df = pd.concat([df_aggregated, df_encode], axis=1)\n",
    "\n",
    "# Add hour from the 'date_time' column\n",
    "df['hour'] = df['date_time'].dt.hour\n",
    "df = df.drop(columns=non_numeric_cols)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:56.485003100Z",
     "start_time": "2023-12-26T09:54:56.466281900Z"
    }
   },
   "id": "7310905065859bfa"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 17)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:56.502628900Z",
     "start_time": "2023-12-26T09:54:56.480331400Z"
    }
   },
   "id": "d8ce7c82090c7c41"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Feature engineering: create lagged and rolling features\n",
    "target = 'traffic_volume'\n",
    "for i in range(1, 4):\n",
    "    df[f'traffic_volume_lag_{i}'] = df[target].shift(i)\n",
    "df['traffic_volume_rolling_mean'] = df[target].rolling(window=3).mean().shift(1)\n",
    "df['traffic_volume_rolling_std'] = df[target].rolling(window=3).std().shift(1)\n",
    "\n",
    "# Remove rows with NaN values resulting from lagged features\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Split the dataset into features and the target\n",
    "X = df.drop(target, axis=1)\n",
    "y = df[target]\n",
    "\n",
    "# Save 'date_time' for later use\n",
    "date_time = df['date_time']\n",
    "\n",
    "# Drop 'date_time' column before scaling\n",
    "df = df.drop(columns=['date_time'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:56.613460200Z",
     "start_time": "2023-12-26T09:54:56.493572600Z"
    }
   },
   "id": "b1eb443b066c3b68"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 21)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:56.636291400Z",
     "start_time": "2023-12-26T09:54:56.526329900Z"
    }
   },
   "id": "90f60372c0e196ea"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['air_pollution_index' 'humidity' 'wind_speed' 'wind_direction'\n",
      " 'visibility_in_miles' 'dew_point' 'temperature' 'rain_p_h' 'snow_p_h'\n",
      " 'clouds_all' 'traffic_volume' 'is_holiday_Christmas Day'\n",
      " 'is_holiday_Columbus Day' 'is_holiday_Independence Day'\n",
      " 'is_holiday_Labor Day' 'is_holiday_Martin Luther King Jr Day'\n",
      " 'is_holiday_Memorial Day' 'is_holiday_New Years Day' 'is_holiday_None'\n",
      " 'is_holiday_State Fair' 'is_holiday_Thanksgiving Day'\n",
      " 'is_holiday_Veterans Day' 'is_holiday_Washingtons Birthday'\n",
      " 'weather_type_Clear' 'weather_type_Clouds' 'weather_type_Drizzle'\n",
      " 'weather_type_Fog' 'weather_type_Haze' 'weather_type_Mist'\n",
      " 'weather_type_Rain' 'weather_type_Snow' 'weather_type_Squall'\n",
      " 'weather_type_Thunderstorm' 'weather_description_SQUALLS'\n",
      " 'weather_description_Sky is Clear' 'weather_description_broken clouds'\n",
      " 'weather_description_drizzle' 'weather_description_few clouds'\n",
      " 'weather_description_fog' 'weather_description_freezing rain'\n",
      " 'weather_description_haze' 'weather_description_heavy intensity drizzle'\n",
      " 'weather_description_heavy intensity rain'\n",
      " 'weather_description_heavy snow'\n",
      " 'weather_description_light intensity drizzle'\n",
      " 'weather_description_light intensity shower rain'\n",
      " 'weather_description_light rain' 'weather_description_light shower snow'\n",
      " 'weather_description_light snow' 'weather_description_mist'\n",
      " 'weather_description_moderate rain' 'weather_description_overcast clouds'\n",
      " 'weather_description_proximity shower rain'\n",
      " 'weather_description_proximity thunderstorm'\n",
      " 'weather_description_proximity thunderstorm with rain'\n",
      " 'weather_description_scattered clouds' 'weather_description_sky is clear'\n",
      " 'weather_description_snow' 'weather_description_thunderstorm'\n",
      " 'weather_description_thunderstorm with heavy rain'\n",
      " 'weather_description_thunderstorm with light drizzle'\n",
      " 'weather_description_thunderstorm with light rain'\n",
      " 'weather_description_very heavy rain' 'hour' 'traffic_volume_lag_1'\n",
      " 'traffic_volume_lag_2' 'traffic_volume_lag_3'\n",
      " 'traffic_volume_rolling_mean' 'traffic_volume_rolling_std']\n"
     ]
    }
   ],
   "source": [
    "scaler = joblib.load('scaler2.joblib')\n",
    "# Check categories in encoder\n",
    "print(scaler.get_feature_names_out())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:56.638291Z",
     "start_time": "2023-12-26T09:54:56.541338600Z"
    }
   },
   "id": "f3e3c7ff3a2f811"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- is_holiday_Christmas Day\n- is_holiday_Columbus Day\n- is_holiday_Independence Day\n- is_holiday_Labor Day\n- is_holiday_Martin Luther King Jr Day\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[22], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Scale the numerical features\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m df_scaled \u001B[38;5;241m=\u001B[39m \u001B[43mscaler\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Use the previously loaded scaler\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;66;03m# Convert scaled data back to DataFrame\u001B[39;00m\n\u001B[0;32m      5\u001B[0m df_scaled \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(df_scaled, columns\u001B[38;5;241m=\u001B[39m[col \u001B[38;5;28;01mfor\u001B[39;00m col \u001B[38;5;129;01min\u001B[39;00m df\u001B[38;5;241m.\u001B[39mcolumns \u001B[38;5;28;01mif\u001B[39;00m col \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdate_time\u001B[39m\u001B[38;5;124m'\u001B[39m])\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\utils\\_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m f(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    163\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\preprocessing\\_data.py:1006\u001B[0m, in \u001B[0;36mStandardScaler.transform\u001B[1;34m(self, X, copy)\u001B[0m\n\u001B[0;32m   1003\u001B[0m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[0;32m   1005\u001B[0m copy \u001B[38;5;241m=\u001B[39m copy \u001B[38;5;28;01mif\u001B[39;00m copy \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcopy\n\u001B[1;32m-> 1006\u001B[0m X \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1007\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1008\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1009\u001B[0m \u001B[43m    \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1010\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcopy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1011\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mFLOAT_DTYPES\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1012\u001B[0m \u001B[43m    \u001B[49m\u001B[43mforce_all_finite\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1013\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1015\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m sparse\u001B[38;5;241m.\u001B[39missparse(X):\n\u001B[0;32m   1016\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mwith_mean:\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\base.py:580\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    509\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_validate_data\u001B[39m(\n\u001B[0;32m    510\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    511\u001B[0m     X\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mno_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    516\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params,\n\u001B[0;32m    517\u001B[0m ):\n\u001B[0;32m    518\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001B[39;00m\n\u001B[0;32m    519\u001B[0m \n\u001B[0;32m    520\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    578\u001B[0m \u001B[38;5;124;03m        validated.\u001B[39;00m\n\u001B[0;32m    579\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 580\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_check_feature_names\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mreset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreset\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    582\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_tags()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires_y\u001B[39m\u001B[38;5;124m\"\u001B[39m]:\n\u001B[0;32m    583\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    584\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThis \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m estimator \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    585\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrequires y to be passed, but the target y is None.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    586\u001B[0m         )\n",
      "File \u001B[1;32m~\\PycharmProjects\\pythonProject1\\v392\\lib\\site-packages\\sklearn\\base.py:507\u001B[0m, in \u001B[0;36mBaseEstimator._check_feature_names\u001B[1;34m(self, X, reset)\u001B[0m\n\u001B[0;32m    502\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m missing_names \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m unexpected_names:\n\u001B[0;32m    503\u001B[0m     message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    504\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFeature names must be in the same order as they were in fit.\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    505\u001B[0m     )\n\u001B[1;32m--> 507\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(message)\n",
      "\u001B[1;31mValueError\u001B[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- is_holiday_Christmas Day\n- is_holiday_Columbus Day\n- is_holiday_Independence Day\n- is_holiday_Labor Day\n- is_holiday_Martin Luther King Jr Day\n- ...\n"
     ]
    }
   ],
   "source": [
    "# Scale the numerical features\n",
    "df_scaled = scaler.transform(df)  # Use the previously loaded scaler\n",
    "\n",
    "# Convert scaled data back to DataFrame\n",
    "df_scaled = pd.DataFrame(df_scaled, columns=[col for col in df.columns if col != 'date_time'])\n",
    "df_scaled['date_time'] = date_time.values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-26T09:54:56.705444700Z",
     "start_time": "2023-12-26T09:54:56.555471800Z"
    }
   },
   "id": "9fde5d3dbbc24acd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_scaled.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T09:54:56.636291400Z"
    }
   },
   "id": "995e1f1c10e67190"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = df_scaled.drop(columns=['date_time'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T09:54:56.638291Z"
    }
   },
   "id": "ffdbcef25df1291a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X = X.drop(target, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T09:54:56.641291700Z"
    }
   },
   "id": "38459d0aaa52d7a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = pd.concat([date_time, df], axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T09:54:56.645334300Z"
    }
   },
   "id": "b6bca2cba1b89271"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T09:54:56.649330900Z"
    }
   },
   "id": "8a499fc70e8d0d83"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Function to update the lagged features with the new prediction\n",
    "def update_lagged_features(df, new_prediction, max_lags=3):\n",
    "    for i in range(max_lags-1, 0, -1):\n",
    "        df[f'traffic_volume_lag_{i+1}'] = df[f'traffic_volume_lag_{i}']\n",
    "    df['traffic_volume_lag_1'] = new_prediction\n",
    "\n",
    "# Initialize DataFrame for dynamic forecasting\n",
    "df_dynamic_forecast = X.copy()\n",
    "df_dynamic_forecast['forecasted_traffic_volume'] = np.nan"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T09:54:56.652861400Z"
    }
   },
   "id": "e7bcef5160dd53b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_dynamic_forecast"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T09:54:56.654494100Z"
    }
   },
   "id": "e03ce6e735c104ff"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of steps to forecast\n",
    "forecast_steps = 2000\n",
    "\n",
    "for i in range(forecast_steps):\n",
    "    # Predict the traffic volume for the next time step\n",
    "    current_prediction = model.predict(df_dynamic_forecast.iloc[i:i+1].drop(columns=['forecasted_traffic_volume']))[0]\n",
    "    df_dynamic_forecast.at[df_dynamic_forecast.index[i], 'forecasted_traffic_volume'] = current_prediction\n",
    "\n",
    "    # Update lagged features with the new prediction for the next iteration\n",
    "    if i + 1 < forecast_steps:\n",
    "        update_lagged_features(df_dynamic_forecast.iloc[i + 1], current_prediction)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T09:54:56.655508800Z"
    }
   },
   "id": "c0c29c49d6fd17d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Initialize the DataFrame\n",
    "test_date_times = df['date_time'].reset_index(drop=True)\n",
    "\n",
    "df_result = pd.DataFrame({\n",
    "    'date_time': test_date_times,\n",
    "    'actual_traffic_volume': y.reset_index(drop=True),\n",
    "    'lag_1': np.nan,\n",
    "    'lag_2': np.nan,\n",
    "    'lag_3': np.nan,\n",
    "    'forecasted_traffic_volume': df_dynamic_forecast['forecasted_traffic_volume'].reset_index(drop=True)\n",
    "})\n",
    "\n",
    "# Set the initial lagged values from the historical data\n",
    "df_result.loc[0, 'lag_1'] = df.loc[df.index[-1], 'traffic_volume']\n",
    "df_result.loc[0, 'lag_2'] = df.loc[df.index[-2], 'traffic_volume']\n",
    "df_result.loc[0, 'lag_3'] = df.loc[df.index[-3], 'traffic_volume']\n",
    "\n",
    "\n",
    "# Update the lagged values with the forecasted values in each step\n",
    "for i in range(1, len(df_result)):\n",
    "    df_result.loc[i, 'lag_1'] = df_result.loc[i - 1, 'forecasted_traffic_volume']\n",
    "    df_result.loc[i, 'lag_2'] = df_result.loc[i - 1, 'lag_1']\n",
    "    df_result.loc[i, 'lag_3'] = df_result.loc[i - 1, 'lag_2']\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_result.head(forecast_steps))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T09:54:56.657508500Z"
    }
   },
   "id": "d56b149dbd26c4f9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-12-26T09:54:56.659517100Z"
    }
   },
   "id": "5d15458e3d0a5a53"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
